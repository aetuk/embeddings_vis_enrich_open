{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4c9de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT MODULES - AZURE OPENAI \n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "# import re\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "# import json\n",
    "# import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3abc5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE ENVIRONMENT VARIABLES\n",
    "OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "#OPENAI_API_KEY = os.getenv('AZURE_GPT4_OPENAI_API_KEY')\n",
    "\n",
    "# DEPLOYMENT_NAME = 'GPT4o128k' # ENTER DEPLOYMENT NAME PER THE AZURE PORTAL DEPLOYMENTS PAGE\n",
    "# DEPLOYMENT_NAME = 'GPT4omini128k' # ENTER DEPLOYMENT NAME PER THE AZURE PORTAL DEPLOYMENTS PAGE\n",
    "# DEPLOYMENT_NAME = 'GPT35Turbo' # ENTER DEPLOYMENT NAME PER THE AZURE PORTAL DEPLOYMENTS PAGE\n",
    "DEPLOYMENT_NAME = 'GPT35Turbo16k' # ENTER DEPLOYMENT NAME PER THE AZURE PORTAL DEPLOYMENTS PAGE\n",
    "# DEPLOYMENT_NAME = 'GPT4-8K' # ENTER DEPLOYMENT NAME PER THE AZURE PORTAL DEPLOYMENTS PAGE\n",
    "# DEPLOYMENT_NAME = 'GPT4Turbo128k' # ENTER DEPLOYMENT NAME PER THE AZURE PORTAL DEPLOYMENTS PAGE\n",
    "\n",
    "OPENAI_API_VERSION = '2024-10-21'  \n",
    "\n",
    "HEADERS = {\n",
    "    'x-service-line': 'CXM', # NEEDS TO BE UPDATED\n",
    "    'x-brand': 'merkle', # NEEDS TO BE UPDATED\n",
    "    'x-project': 'StarterGPTDemo', # NEEDS TO BE UPDATED\n",
    "    'Content-Type': 'application/json',\n",
    "    'Cache-Control': 'no-cache',\n",
    "    'Ocp-Apim-Subscription-Key': OPENAI_API_KEY,\n",
    "    'api-version': 'v15'\n",
    "}\n",
    "\n",
    "LLM = AzureOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_endpoint='https://ai-api-dev.dentsu.com/',\n",
    "    default_headers=HEADERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31485657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE PROMPT\n",
    "prompt = f'''\n",
    "\n",
    "How many days are there in December?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0151b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE CHAT COMPLETION\n",
    "response = LLM.chat.completions.create(\n",
    "    model= DEPLOYMENT_NAME,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You're a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many days are there in December?\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    top_p=0.9, #Higher values like 0.9 allow more tokens, leading to diverse responses, while lower values like 0.2 provide more focused and constrained answers.\n",
    "    frequency_penalty=0.6, #Higher values penalize the model for repeating the same response, while lower values encourage repetition.\n",
    "    presence_penalty=0.6, #Higher values encourage the model to talk about new topics, while lower values encourage the model to repeat itself.\n",
    "    max_tokens=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fdde555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How many days are there in December?\n",
      "--------------------------------------------\n",
      "Response: How can I assist you today?\n",
      "--------------------------------------------\n",
      "Prompt Tokens: 13\n",
      "Completion Tokens: 7\n",
      "Total Tokens: 20\n"
     ]
    }
   ],
   "source": [
    "#EXTRACT RESPONSE AND PRINT THE RESULTS\n",
    "content = response.choices[0].message.content\n",
    "prompt_tokens = response.usage.prompt_tokens\n",
    "completion_tokens = response.usage.completion_tokens\n",
    "total_tokens = response.usage.total_tokens\n",
    "#################\n",
    "print(\"Prompt:\", prompt.strip())\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Response:\", content)\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Prompt Tokens:\", prompt_tokens)\n",
    "print(\"Completion Tokens:\", completion_tokens)\n",
    "print(\"Total Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6e335-77df-45b8-906f-8b543ae7b972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
